name: "XOR"

# Training data input
layer {
    name: "data_input"
    type: "MemoryData"
    top: "fulldata"
    top: "fakelabel"
    include {
        phase: TRAIN
    }
    memory_data_param {
        batch_size: 64
        channels: 1
        height: 1
        width: 2        
    }
}

# Testing data input
layer {
    name: "test_inputdata"
    type: "MemoryData"
    top: "fulldata"
    top: "fakelabel"
    include {
        phase: TEST
    }
    memory_data_param {
        batch_size: 4 
        channels: 1 
        height: 1
        width: 2 
    }
}

layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "fulldata"
    top: "fc6"
    inner_product_param {
        num_output: 2
        weight_filler {
            type: "xavier"
        }
        bias_filler {
            type: "constant"
        }
    }
}

layer {
    name: "fc6sig"
    bottom: "fc6"
    top: "fc6"
    type: "Sigmoid"
}

layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    inner_product_param {
        num_output: 1
        weight_filler {
            type: "xavier"
        }
        bias_filler {
            type: "constant"
        }
    }
}

# Output for testing: just sigmoid it
layer {
    name: "output"
    bottom: "fc7"
    top: "output"
    type: "Sigmoid"
    include {
        phase: TEST
    }
}

# Output for training: activated by sigmoid and cost computed by cross entropy
layer {
    name: "loss"
    type: "SigmoidCrossEntropyLoss"
    bottom: "fc7"
    bottom: "fakelabel"
    top: "loss"
}